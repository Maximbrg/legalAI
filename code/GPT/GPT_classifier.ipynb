{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt few-shot classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "import uuid  \n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wep examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples = {\n",
    "    \"CONFESSION\": [\n",
    "        {\"text\": \"הנאשם הורשע עפ\\\"י הודייתו, במסגרת הסדר טיעון בגדרו הוגש כתב אישום מתוקן, בעבירה שעניינה החזקת נשק ותחמושת שלא כדין, לפי סעיף 144(א) לחוק העונשין, תשל\\\"ז – 1977 (להלן: 'חוק העונשין').\", \"label\": 1},\n",
    "        {\"text\": \"הסנגור עמד על נסיבות חייו הקשות של הנאשם, אובדנה של בתו לפני מספר שנים ובעיותיו הרפואיות, הודייתו בעבירה בהזדמנות הראשונה וחרטתו הכנה על מעשיו.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם הודה בעובדות כתב אישום שעניינן, נשיאת והחזקת נשק שלא כדין.\", \"label\": 1},\n",
    "        {\"text\": \"תחילה, כפר הנאשם במיוחס לו בכתב האישום.\", \"label\": 1},\n",
    "        {\"text\": \"בדיון מיום 18/11/13 ולאחר שהוחל בשמיעת ראיות, הגיעו הצדדים להסדר טיעון ולפיו, הנאשם יחזור בו מכפירתו, כתב האישום יתוקן, הנאשם יודה בעובדות כתב האישום המתוקן ויורשע על פי הודאתו.\", \"label\": 1}\n",
    "    ],\n",
    "    \"CIR_TYPE_WEP\": [\n",
    "        {\"text\": \"הנאשם הודה, כי נשא אקדח .\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם הורשע על סמך הודאתו בהסדר בכתב אישום מתוקן בהחזקת  רימון הלם.\", \"label\": 1},\n",
    "        {\"text\": \"במסגרת העבירה נמכר תת מקלע מסוג קרלו.\", \"label\": 1},\n",
    "        {\"text\": \"לאחר מספר דקות עצר עג\\\"אג\\' את הרכב, השלושה יצאו ממנו ועג\\\"אג\\' הוציא מתא המטען רובה סער מסוג M-4 ובכוחו להמית אדם (להלן: 'הרובה'), מחסנית תואמת ושקית ובה מספר רב של כדורים.\", \"label\": 1},\n",
    "        {\"text\": \"כמו כן, הודה הנאשם, כי החזיק מתחת לכיסא הנהג שברכבו שקית ובה 17 כדורי אקדח ומחסנית לאקדח.\", \"label\": 1},\n",
    "\n",
    "    ],\n",
    "    \"CIR_HELD_WAY_WEP\": [\n",
    "        {\"text\": \"הנאשם הודה, כי נשא אקדח הטעון במחסנית ריקה ובכיסו האחורי נמצאו 2 כדורים התואמים לאקדח.\", \"label\": 1},\n",
    "        {\"text\": \"מתחת למושב הקדמי הימני נשא הנאשם מחסנית שבתוכה כדורים.\", \"label\": 1},\n",
    "        {\"text\": \"באשר לסוגיית התכנון שקדם לביצוע העבירות והאם עסקינן במעשים אשר הינם פרי תכנון מוקדם, או מעשים רגעיים וספונטניים, נציין מחד, את עובדות כתב האישום המתוקן אשר מהן עולה, כי במועד אשר אינו ידוע למאשימה, הצטייד הנאשם בנשק מסוג אקדח וכי על רקע האירועים הקשורים לסכסוך, הגיע הנאשם למרכז הכפר כשהוא נושא ומוביל עמו את הנשק כשהוא טעון.\", \"label\": 1},\n",
    "        {\"text\": \"על פי שירות המבחן, עת התייחס הנאשם לעבירה הנוכחית, הוא לקח אחריות על מעשיו והסביר כי נשא ברכבו אקדח, לא טעון וללא רישיון, נוכח תחושת איום ממשית לחייו ולחיי משפחתו, על רקע סכסוך עם אחרים שחשדו כי היה מעורב ברצח של מקורב אליהם.\", \"label\": 1},\n",
    "        {\"text\": \"מאידך ראוי לציין, כי כעולה מעובדות כתב האישום, הרי שבפועל לא נעשה שימוש בנשק ולא נגרם כל נזק ממשי ומוחשי בעטיו, מבלי להקל ראש בעצם נשיאת הנשק, החזקת האקדח ברחובה של עיר כשהוא טעון ובנוכחות אחרים, עמם נתון הנאשם בסכסוך.\", \"label\": 1}\n",
    "    ],\n",
    "    \"CIR_AMMU_AMOUNT_WEP\": [\n",
    "        {\"text\": \"הנאשם הודה, כי נשא אקדח הטעון במחסנית ריקה ובכיסו האחורי נמצאו 2 כדורים התואמים לאקדח.\", \"label\": 1},\n",
    "        {\"text\": \"כמו כן, הודה הנאשם, כי החזיק מתחת לכיסא הנהג שברכבו שקית ובה 17 כדורי אקדח ומחסנית .\", \"label\": 1},\n",
    "        {\"text\": \"מתחת למושב הקדמי הימני נשא הנאשם את האקדח ובו מחסנית ריקה.\", \"label\": 1},\n",
    "        {\"text\": \"זאת עשה הנאשם בלא רשות על פי דין לנשיאתם והחזקתם של האקדח והמחסנית.\", \"label\": 1},\n",
    "        {\"text\": \"כמו כן, הודה הנאשם, כי החזיק מתחת לכיסא הנהג שברכבו שקית ובה 17 כדורי אקדח ומחסנית לאקדח.\", \"label\": 1}\n",
    "    ],\n",
    "    \"CIR_PURPOSE\": [\n",
    "        {\"text\": \"בעת הצגת ההסדר ציינו ב\\\"כ הצדדים, כי אין מחלוקת שהנאשם החזיק בנשק לצורך הגנה עצמית.\", \"label\": 1},\n",
    "        {\"text\": \"בהקשר זה ראוי לציין, כי הנאשם החזיק שלא כדין אקדח והגיע עמו לטבורו של כפר, כשהוא טעון, זאת במטרה להפחיד אחרים המעורבים בסכסוך ואשר היו בקרבת מקום.\", \"label\": 1},\n",
    "        {\"text\": \"באשר לנזק שצפוי היה להיגרם מביצוע העבירות, יצוין בהקשר זה האמור בעובדות כתב האישום המתוקן ולפיהן, הנאשם הגיע עם הנשק כשהוא טעון, במטרה להפחיד את המעורבים בסכסוך אשר היו בקרבת מקום, על המשמעויות הרוחביות הקמות מכך, באשר לנזק שעלול היה להיגרם מהחזקת אקדח כשהוא טעון ברחובה של עיר, בנוכחות אחרים, אשר מעורבים בסכסוך עם הנאשם ואחרים.\", \"label\": 1},\n",
    "        {\"text\": \"כמו כן, לשיטתה העובדה כי האקדח נמצא טעון במחסנית מעידה על כך כי לא הוחזק ל\\\"מטרות שלום\\\", כדבריה.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם הטעים כי תחושות הסכנה והפחד התגברו לאחר הפגיעה בבנו, או אז, גמלה בלבו ההחלטה להשיג כלי נשק, למען הגנתו.\", \"label\": 1}\n",
    "    ],\n",
    "    \"GENERAL_CIRCUM\": [\n",
    "        {\"text\": \"הנאשם שייך גם את מעורבותו בעבירות דנן וגם את אורח החיים שניהל לגילו הצעיר, לנטייתו לחפש אחר חופש, שייכות בקרב קבוצת השווים וחוסר התחשבות בגבולות.\", \"label\": 1},\n",
    "        {\"text\": \"שירות המבחן ציין כי ההליך הפלילי היווה גורם מרתיע לנאשם.\", \"label\": 1},\n",
    "        {\"text\": \"הסנגור עמד על נסיבות חייו הקשות של הנאשם, אובדנה של בתו לפני מספר שנים ובעיותיו הרפואיות, הודייתו בעבירה בהזדמנות הראשונה וחרטתו הכנה על מעשיו.\", \"label\": 1},\n",
    "        {\"text\": \"זאת ועוד, הנאשם נמנע מחיי חברה שוליים וכפי העולה מתסקיר שירות המבחן, הוא ביטא כוחות תוך שמירה על גבולות, הודה במיוחס לו, לקח אחריות על המעשים והביע חרטה מלאה וכנה בשלהם.\", \"label\": 1},\n",
    "        {\"text\": \"בנסיבות אלו, נוכח מקבץ הנסיבות אשר אינן קשורות בביצוע העבירות ובכלל זה, האמור בליבת תסקירי שירות המבחן שהוגשו בעניינו של הנאשם, האמור בתסקיר הראשון וההמלצה המובאת בסופו, מנגד, השוני באמור בתסקירים המשלימים שהוגשו וההמלצה העדכנית המובאת בתסקיר המסכם שהוגש, כמו גם הודאת הנאשם, לקיחת האחריות, חיסכון זמן שיפוטי יקר, הצער והחרטה שהביע הנאשם בשל המעשים, נסיבותיו האישיות והמשפחתיות של הנאשם, העובדה, כי עסקינן בנאשם צעיר נעדר כל הרשעות קודמות ואשר זוהי לו עשייתו הראשונה בפלילים, באתי לכלל מסקנה, כי יש למקם את העונש הראוי לנאשם בתוך מתחם העונש ההולם שקבענו לעיל, ברף הנמוך של מתחם העונש ההולם אך לא בצד הקיצון שבו.\", \"label\": 1}\n",
    "    ],\n",
    "    \"CIR_STATUS_WEP\": [\n",
    "        {\"text\": \"  בנוסף על פי העבירה ניתן לומר שהנאשם ביצע ירי באזור מגורים.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם הודה, כי נשא אקדח הטעון במחסנית ריקה ובכיסו האחורי נמצאו 2 כדורים התואמים לאקדח.\", \"label\": 1},\n",
    "        {\"text\": \"מדובר בכלי נשק שמסוגל לירות כדור שבכוחו להמית אדם\", \"label\": 0},\n",
    "        {\"text\": \"בנוסף, יש לקחת בחשבון, את העובדה כי הנשק אינו תקין.\", \"label\": 1},\n",
    "        {\"text\": \"יצוין, כי דבריו אלו של הנאשם, עומדים הם במידה מסוימת בסתירה לעובדות כתב האישום, שעה שמהעובדות עולה כאמור, כי הנאשם הצטייד באקדח והגיע למרכז הכפר כשהוא נושא ומוביל עמו את הנשק כשהוא טעון.\", \"label\": 1}\n",
    "    ],\n",
    "    \"REGRET\": [\n",
    "        {\"text\": \"הסנגור עמד על נסיבות חייו הקשות של הנאשם, אובדנה של בתו לפני מספר שנים ובעיותיו הרפואיות, הודייתו בעבירה בהזדמנות הראשונה וחרטתו הכנה על מעשיו.\", \"label\": 1},\n",
    "        {\"text\": \"זאת ועוד, הנאשם נמנע מחיי חברה שוליים וכפי העולה מתסקיר שירות המבחן, הוא ביטא כוחות תוך שמירה על גבולות, הודה במיוחס לו, לקח אחריות על המעשים והביע חרטה מלאה וכנה בשלהם.\", \"label\": 1},\n",
    "        {\"text\": \"בנסיבות אלו, נוכח מקבץ הנסיבות אשר אינן קשורות בביצוע העבירות ובכלל זה, האמור בליבת תסקירי שירות המבחן שהוגשו בעניינו של הנאשם, האמור בתסקיר הראשון וההמלצה המובאת בסופו, מנגד, השוני באמור בתסקירים המשלימים שהוגשו וההמלצה העדכנית המובאת בתסקיר המסכם שהוגש, כמו גם הודאת הנאשם, לקיחת האחריות, חיסכון זמן שיפוטי יקר, הצער והחרטה שהביע הנאשם בשל המעשים, נסיבותיו האישיות והמשפחתיות של הנאשם, העובדה, כי עסקינן בנאשם צעיר נעדר כל הרשעות קודמות ואשר זוהי לו עשייתו הראשונה בפלילים, באתי לכלל מסקנה, כי יש למקם את העונש הראוי לנאשם בתוך מתחם העונש ההולם שקבענו לעיל, ברף הנמוך של מתחם העונש ההולם אך לא בצד הקיצון שבו.\", \"label\": 1},\n",
    "        {\"text\": \"מנגד, אף עמדתה העונשית של המאשימה, חורגת היא לחומרא, אינה עולה בקנה אחד עם מתחם העונש ההולם שקבענו ועם מיקום העונש הראוי לנאשם ואינה משקללת נכונה את נתוני העושה, נסיבותיו האישיות, היות הנאשם נעדר עבר פלילי, גילו הצעיר, הודאת הנאשם, הצער והחרטה שהביע בשל המעשים, כמו גם ההליך הטיפולי בו החל ליטול חלק והמלצת שירות המבחן כפי תסקירו האחרון.\", \"label\": 1},\n",
    "        {\"text\": \"הסנגור ציין כי ההגנה אינה מקלה ראש בעבירה דנן, ומודעת לסכנה הטמונה בעבירות נשק, אך ביקש להתחשב לקולא בגזירת עונשו בנסיבות ביצוע העבירה, בנסיבותיו האישיות של הנאשם, בהודייתו בביצוע העבירה ובחרטתו הכנה והאמיתית.\", \"label\": 1}\n",
    "    ],\n",
    "\"PUNISHMENT\": [\n",
    "    {\"text\": \"12 חודשי מאסר על תנאי והתנאי הוא שהנאשם לא יעבור תוך תקופה של 3 שנים כל עבירה בנשק מסוג פשע ויורשע בגינה.\", \"label\": 1},\n",
    "    {\"text\": \"12 חודשי מאסר על תנאי למשך 3 שנים והתנאי הוא שהנאשם לא יעבור עבירה בנשק.\", \"label\": 1},\n",
    "    {\"text\": \"א.\\t9 חודשי מאסר בפועל בניכוי ימי מעצרו מיום 8/4/13 ועד ליום 11/7/13.\", \"label\": 1},\n",
    "    {\"text\": \"עבודות במסגרת של\\\\\\\"צ בהיקף של 500 שעות, עפ\\\\\\\"י תכנית שתוכן ע\\\\\\\"י שירות המבחן ותומצא בכתב לבית המשפט.\", \"label\": 1},\n",
    "    {\"text\": \"6 חודשי מאסר על תנאי והתנאי הוא שהנאשם לא יעבור תוך תקופה של 3 שנים כל עבירה בנשק מסוג עוון ויורשע בגינה.\", \"label\": 1}\n",
    "],\n",
    "    \"CIR_PLANNING\": [\n",
    "        {\"text\": \"באשר לסוגיית התכנון שקדם לביצוע העבירות והאם עסקינן במעשים אשר הינם פרי תכנון מוקדם, או מעשים רגעיים וספונטניים, נציין מחד, את עובדות כתב האישום המתוקן אשר מהן עולה, כי במועד אשר אינו ידוע למאשימה, הצטייד הנאשם בנשק מסוג אקדח וכי על רקע האירועים הקשורים לסכסוך, הגיע הנאשם למרכז הכפר כשהוא נושא ומוביל עמו את הנשק כשהוא טעון.\", \"label\": 1},\n",
    "        {\"text\": \"מעשה כזה, כך לטענת המאשימה, הינו מעשה מתוכן וכי הנאשם אף מודה, כי הייתה בהחזקת הנשק הטעון כוונה להפחיד ואף ציין כיצד התכוון להפחיד את הזולת כשהנשק טעון.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם תכנן את מעשיו מבעוד מועד.\", \"label\": 1},\n",
    "        {\"text\": \"נטען שאין מדובר בטעות בשיקול דעת או במעידה רגעית כאשר הנאשם הגיע במודע הן למקום העבודה והן לאחר מכן לבית המתלונן כאשר הוא מצוייד בנשק בו הוא עשה שימוש.\", \"label\": 1},\n",
    "        {\"text\": \"לגבי נסיבות ביצוע העבירה, ציינה כי מדובר במעשה שבוצע לאחר תכנון מוקדם, תכנון מתמשך לאחר התחברות לאחר.\", \"label\": 1},\n",
    "  ]  ,\n",
    "    \"RESPO\": [\n",
    "        {\"text\": \"באשר לעבירות דנן, הרי שהנאשם התקשה לקבל אחריות לעובדות כתב האישום המתוקן.\", \"label\": 1},\n",
    "        {\"text\": \"הודיה זו לא זו בלבד שהובילה לחסכון בזמן שיפוטי יקר, אלא גם מגלמת בתוכה נטילת אחריות על מעשיו.\", \"label\": 1},\n",
    "        {\"text\": \"שירות המבחן לא נתן אמון מלא בגרסתו והעריך שנטילת האחריות שלו היא חלקית בלבד.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם הביע חרטה על מעשיו ולקח עליהם אחריות.\", \"label\": 1},\n",
    "        {\"text\": \"זאת ועוד, הנאשם לקח אחריות על המעשים .\", \"label\": 1},\n",
    "\n",
    "\n",
    "    ],\n",
    "        \"CIR_USE\": [\n",
    "        {\"text\": \"הגיע הנאשם למקום, ויידה שלושה בקבוקי תבערה, ממרחק של 20 מטר, לעבר הרכבים של כוחות הביטחון.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם הורשע בעבירה של ירי באזור מגורים.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם ניסה לירות בנשק המאולתר לעבר רגליו של אחד מהאחרים, אך הנשק לא פעל.\", \"label\": 1},\n",
    "        {\"text\": \"במועד האירוע הושלך הרימון לעבר מרפסת ביתו של המתלונן, לאחר שהנצרה שוחררה.\", \"label\": 1},\n",
    "        {\"text\": \"מעבר להחזקתו בתת המקלע המאולתר, הודה הנאשם כי במהלך שנת 2011 הובילו ונשאו למקום פתוח, שם ירה באמצעותו כ-20 כדורים.\", \"label\": 1},\n",
    "    ],\n",
    "    \"CIR_OBTAIN_WAY_WEP\": [\n",
    "        {\"text\": \"מנגד, נציין את דברי הנאשם בפני שירות המבחן, כפי שהדבר קיבל את ביטויו בתסקירי שירות המבחן שהוגשו, בדבר קבלת האקדח במקום האירוע במרכז הכפר, ממאן דהוא, אשר הנאשם אינו זוכר את זהותו ושעה שהנאשם היה נתון תחת השפעת אלכוהול.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם הכחיש, כי הוא הגיע לקטטה כשהוא מצויד באקדח ולטענתו, האקדח ניתן לו בעת האירוע מאדם שאינו זוכר, בשל היותו תחת השפעת אלכוהול.\", \"label\": 1},\n",
    "        {\"text\": \"באישום השלישי החזיק הנאשם אקדח שקיבל מאביו, בביתו של בן דודו בכפר עקב, וביום סיום הבגרויות לקח את האקדח וירה שמונה כדורים באוויר.\", \"label\": 1},\n",
    "        {\"text\": \"לאחר מכן קיבל הנאשם מאביו אקדח בלגי, אותו החזיק בבית של בן דודו.\", \"label\": 1},\n",
    "        {\"text\": \"במועד כלשהו שוחח הנאשם עם סאמי וזאת כדי לרכוש ממנו נשק.\", \"label\": 0},\n",
    "        {\"text\": \"באשר לעבירה, הנאשם ייצר את המטען.\", \"label\": 1}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_examples_drug = {\n",
    "    \"CIR_TYPE\": [\n",
    "        {\"text\": \"בהמשך נפגשו השניים בפארק, אז מכר לו הנאשם גרם קנבוס בתמורה ל-120 ₪.\", \"label\": 1},\n",
    "        {\"text\": \"ביום 20.10.22 שב הנאשם לישראל כשהוא נושא עמו כ-25,303 טבליות סם מסוכן מסוג MDMA שהוטמנו בתא ייעודי שבנה בתחתית מזוודתו, אותו קיבע באמצעות קרש עץ.\", \"label\": 1},\n",
    "        {\"text\": \"עמית הסכים, אך ביקש כי במקום תשלום של 5,000 ₪ ייתן לו הנאשם סם מסוכן מסוג קטמין בשווי של 5,000 ₪ הנאשם הסכים ואמר לו כשיגיעו לארץ, יתחשבנו.\", \"label\": 1},\n",
    "        {\"text\": \"כעבור מספר דקות, הגיע גם הנאשם למקום על אופניו, ניגש לעבר חלון רכבו של הסוכן ומסר לו גוש עטוף בנייר ובו 99.53 גרם נטו של סם מסוכן מסוג קוקאין.\", \"label\": 1},\n",
    "        {\"text\": \"לפי המתואר בכתב האישום, ביום 14.04.23 גמלה בליבו של הנאשם החלטה לייבא לארץ סם מסוכן מסוג GHB.\", \"label\": 1},\n",
    "\n",
    "    ],\n",
    "    \"CIR_AMOUNT\": [\n",
    "        {\"text\": \"ביום 20.10.22 שב הנאשם לישראל כשהוא נושא עמו כ-25,303 טבליות סם מסוכן מסוג MDMA שהוטמנו בתא ייעודי שבנה בתחתית מזוודתו, אותו קיבע באמצעות קרש עץ.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם פתח את החבילה והוציא ממנה את שני הכדים שהכילו כ- 350 גרם נטו של סם מסוכן מסוג KETAMINE מסר אותם לידי עמית ואמר לו לשים אותם במושב הנהג בהונדה.\", \"label\": 1},\n",
    "        {\"text\": \"בתאריך 14.8.22 בשעה 14:02, הגיע הסוכן למקום ושם פגש את הנאשם שהעביר לו עטיפת נייר ובה שקית המכילה גוש של סם מסוכן מסוג קוקאין במשקל 119.89 גרם נטו.\", \"label\": 1},\n",
    "        {\"text\": 'במעבדה נתפסו 137.6 ק\"ג של סם מסוכן מסוג קנביס;', \"label\": 1},\n",
    "        {\"text\": 'בתיק המצורף החזיק הנאשם, לצריכתו העצמית, בסם מסוכן מסוג קנביס במשקל של 98.6 גרם.', \"label\": 1}\n",
    "    ],\n",
    "    \"GENERAL_CIRCUM\": [\n",
    "        {\"text\": \"הנאשם שייך גם את מעורבותו בעבירות דנן וגם את אורח החיים שניהל לגילו הצעיר, לנטייתו לחפש אחר חופש, שייכות בקרב קבוצת השווים וחוסר התחשבות בגבולות.\", \"label\": 1},\n",
    "        {\"text\": \"עוד עולה, כי עד להסתבכותו של הנאשם במקרה דנן, ניהל הנאשם אורח חיים פרודוקטיבי ונורמטיבי ועשה לביתו ולפרנסתו.\", \"label\": 1},\n",
    "        {\"text\": \"הסנגור עמד על נסיבות חייו הקשות של הנאשם, אובדנה של בתו לפני מספר שנים ובעיותיו הרפואיות, הודייתו בעבירה בהזדמנות הראשונה וחרטתו הכנה על מעשיו.\", \"label\": 1},\n",
    "        {\"text\": \"זאת ועוד, הנאשם נמנע מחיי חברה שוליים וכפי העולה מתסקיר שירות המבחן, הוא ביטא כוחות תוך שמירה על גבולות, הודה במיוחס לו, לקח אחריות על המעשים והביע חרטה מלאה וכנה בשלהם.\", \"label\": 1},\n",
    "        {\"text\": \"בנסיבות אלו, נוכח מקבץ הנסיבות אשר אינן קשורות בביצוע העבירות ובכלל זה, האמור בליבת תסקירי שירות המבחן שהוגשו בעניינו של הנאשם, האמור בתסקיר הראשון וההמלצה המובאת בסופו, מנגד, השוני באמור בתסקירים המשלימים שהוגשו וההמלצה העדכנית המובאת בתסקיר המסכם שהוגש, כמו גם הודאת הנאשם, לקיחת האחריות, חיסכון זמן שיפוטי יקר, הצער והחרטה שהביע הנאשם בשל המעשים, נסיבותיו האישיות והמשפחתיות של הנאשם, העובדה, כי עסקינן בנאשם צעיר נעדר כל הרשעות קודמות ואשר זוהי לו עשייתו הראשונה בפלילים, באתי לכלל מסקנה, כי יש למקם את העונש הראוי לנאשם בתוך מתחם העונש ההולם שקבענו לעיל, ברף הנמוך של מתחם העונש ההולם אך לא בצד הקיצון שבו.\", \"label\": 1}\n",
    "    ],\n",
    "\"PUNISHMENT\": [\n",
    "    {\"text\": \"12 חודשי מאסר על תנאי והתנאי הוא שהנאשם לא יעבור תוך תקופה של 3 שנים כל עבירה בנשק מסוג פשע ויורשע בגינה.\", \"label\": 1},\n",
    "    {\"text\": \"12 חודשי מאסר על תנאי למשך 3 שנים והתנאי הוא שהנאשם לא יעבור עבירה בנשק.\", \"label\": 1},\n",
    "    {\"text\": \"א.\\t9 חודשי מאסר בפועל בניכוי ימי מעצרו מיום 8/4/13 ועד ליום 11/7/13.\", \"label\": 1},\n",
    "    {\"text\": \"עבודות במסגרת של\\\\\\\"צ בהיקף של 500 שעות, עפ\\\\\\\"י תכנית שתוכן ע\\\\\\\"י שירות המבחן ותומצא בכתב לבית המשפט.\", \"label\": 1},\n",
    "    {\"text\": \"6 חודשי מאסר על תנאי והתנאי הוא שהנאשם לא יעבור תוך תקופה של 3 שנים כל עבירה בנשק מסוג עוון ויורשע בגינה.\", \"label\": 1}\n",
    "],\n",
    "        \"CIR_ROLE\": [\n",
    "        {\"text\": \"הנאשם הינו הגורם המרכזי, המתכנן והמוציא אל הפועל של העבירה, תוך הסתייעות באחר והפעלתו.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם ייבא סם מסוכן תוך תכנון, הסלקה והפעלת גורמים נוספים, כאשר מדובר בסם מסוכן בכמות גדולה, סם אשר כאמור בת.פ.\", \"label\": 1},\n",
    "        {\"text\": \"באותן הנסיבות שימש המתלונן כשליח של השניים לצורך ביצוע עסקאות סם.\", \"label\": 1},\n",
    "        {\"text\": \"ובמקרה דנן, מעמדו של הנאשם כמתווך נמצא גבוה בהיררכיה של שרשרת הפצת הסם.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם ביצע את כל העסקאות באופן עצמאי וללא כל עזרה של גורם מתווך כלשהו.\", \"label\": 1}\n",
    "    ],\n",
    "    \"CIR_EQ\": [\n",
    "        {\"text\": \"בנוסף לאמור, נתפס בוילה ציוד רב ששימש לגידול הסם.\", \"label\": 1},\n",
    "        {\"text\": \"לשם כך התקין בה ציוד להפקה, הכנה וטיפול בקנבוס, כולל גופי תאורה ייעודיים, מערכות אוורור והשקיה, אדניות וחומרי דישון.\", \"label\": 1},\n",
    "        {\"text\": 'ב\"כ המאשימה טענה לעניין כתב האישום כי מדובר בתעוזה, מעבדה שהנאשם הקים בדירת מגוריו, שם ילדיו גרים, שם מקיים חיי משפחה, ומצייד אותה בציוד יקר ערך.', \"label\": 1},\n",
    "        {\"text\": \"לטענתו, אין המדובר במעבדה קטנה, זניחה ומאולתרת, אלא במעבדת סמים משוכללת.\", \"label\": 1},\n",
    "        {\"text\": \"בענייננו, הקים הנאשם מעבדת סמים במחסן בחצר ביתו ורכש ציוד ייעודי וכלים שונים וביניהם מד טמפרטורה, מאווררים, צינורות השקיה, מנורות חימום, דישון ועוד – הכל לצורך גידול הסם והכנתו.\", \"label\": 1}\n",
    "    ],\n",
    "        \"REGRET\": [\n",
    "        {\"text\": \"בהודאתו, חסך הנאשם זמן שיפוטי והביע חרטה.\", \"label\": 1},\n",
    "        {\"text\": \"שירות המבחן ציין, כי הנאשם לקח אחריות מלאה למעשיו ומצר על ביצוע העבירה, ומודע כיום לנזקים וההשלכות הכרוכים בביצועה.\", \"label\": 1},\n",
    "        {\"text\": \"בהתייחסותו לעבירה הנוכחית הוא לקח אחריות, ביטא צער וחרטה בגינם לצד הכרה בחומרתם.\", \"label\": 1},\n",
    "        {\"text\": \"בדבריו לבית המשפט הנאשם הביע צער על מעשיו, התנצל, אמר שהוא מעוניין לגדל את בנו, לחיות איתו, להיות אבא טוב ולעבור שינוי.\", \"label\": 1},\n",
    "        {\"text\": \"עוד התרשם שירות המבחן, כי הנאשם מבטא מודעות למצבו, מביע חרטה כנה על האופן שבו פעל, ואף הביא דוגמאות מחייו כיום המעידות על שינוי דפוסים.\", \"label\": 1}\n",
    "    ],\n",
    "    \"RESPO\": [\n",
    "        {\"text\": \"כ הנאשם ציין: המדובר בנאשם צעיר שלקח אחריות, הודה וחסך זמן שיפוטי ניכר.\", \"label\": 1},\n",
    "        {\"text\": \"מידת הפגיעה בערך המוגן; גילו של הנאשם; הנסיבות עליהן עמדו הצדדים בטיעוניהם; הודאתו המצביעה על נטילת אחריות.\", \"label\": 1},\n",
    "        {\"text\": \"הנאשם עצמו הצטער על מעשיו, קיבל אחריות עליהם, וציין כי הוא חש בושה על ביצוע העבירות.\", \"label\": 1},\n",
    "        {\"text\": \"מתסקיר שירות המבחן עולה שהנאשם עבר תהליך של חקירה עצמית עמוקה ושינוי מחשבתי, מקפיד לשמור על החוק והסדר החברתי ומבטא נכונות להשתלב בטיפול.\", \"label\": 1},\n",
    "        {\"text\": \"לקולה אני מביא בחשבון את: קבלת האחריות; הבעת החרטה; ההודאה בכתב האישום המתוקן, החיסכון בזמן שיפוטי רב.\", \"label\": 1}\n",
    "    ],\n",
    "    \"CONFESSION\": [\n",
    "        {\"text\": \"ביום 27.11.22 הגיעו הצדדים להסדר לפיו, הנאשם הודה והורשע בעובדות כתב האישום.\", \"label\": 1},\n",
    "        {\"text\": \"לזכותו של הנאשם המאשימה זקפה את הודאתו.\", \"label\": 1},\n",
    "        {\"text\": \"עוד הדגיש את התנהלותו לאחר תפיסתו, לרבות הודאתו בשלב מוקדם, הירתמותו הטוטאלית להליך טיפולי אינטנסיבי בקהילה לגמילה מסמים.\", \"label\": 1},\n",
    "        {\"text\": \"לא מצאתי לחרוג ממתחם העונש ההולם אותו קבעתי ובגדרי המתחם נתתי משקל להודאתו של הנאשם ולחיסכון בזמן שיפוטי.\", \"label\": 1},\n",
    "        {\"text\": \"עם זאת, יש בשיקולים לקולא, ובעיקר ההודיה, חלקו של הנאשם בשרשרת, נסיבות גיוסו והעובדה כי זהו מאסרו הראשון.\", \"label\": 1}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wep Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HebrewLegalMultiLabelClassifierWep:\n",
    "    \"\"\"\n",
    "    A Hebrew legal text classifier using OpenAI's GPT models.\n",
    "    This classifier supports zero-shot, few-shot, and chain-of-thought prompting techniques\n",
    "    to determine whether a given legal text contains specific predefined labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_path: str, model_name: str = \"gpt-4o-mini\"):\n",
    "        \"\"\"\n",
    "        Initializes the classifier with the specified model and output path.\n",
    "        \n",
    "        Args:\n",
    "            output_path (str): Path to save classification results.\n",
    "            model_name (str, optional): OpenAI model name. Defaults to \"gpt-4o-mini\".\n",
    "        \"\"\"\n",
    "\n",
    "        self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.model_name = model_name\n",
    "        self.output_path = output_path\n",
    "        self.label_descriptions = {\n",
    "            'CONFESSION': \"האם מופיעה הודאה או הכחשה מפורשת של ביצוע העבירה? תייג נכון אם מופיע ביטוי שמעיד על כך שהנאשם הודה או כפר במעשיו לדוגמא : הודייתו, הודאה,כפירתו וכו...\",\n",
    "            'CIR_TYPE_WEP': 'האם מופיע סוג הנשק המעורב בעבירה? יש לתייג נכון אם הטקסט מציין מפורשות סוג ספציפי של נשק, כמו: רובים ואקדחים (רובה סער, תת-מקלע, אקדח, קלצניקוב, וכו), חומרי נפץ (מטען חבלה, רימון, בקבוק תבערה, מטען צינור וכו). אין לתייג נכון אם: הטקסט מתאר שימוש בנשק מבלי לציין סוג ספציפי,הטקסט משתמש במונחים כלליים כמו נשק חם, נשק התקפי, נשק אוטומטי ללא פירוט נוסף',\n",
    "            'CIR_HELD_WAY_WEP': 'האם מופיע תיאור ברור לאופן החזקת הנשק? יש לתייג \"נכון\" אם הטקסט מתאר היכן הנשק עצמו הוחזק (למשל: בבית, ברכב, על גופו, מוסתר במקום מסוים) או כיצד הוסתר/נשא אותו. ניתן לכלול גם אביזרים נלווים (מחסנית, תחמושת וכו) בתנאי שהתיאור כולל גם את הנשק עצמו. אין לתייג \"נכון\" אם טקסט מתאר רק את עצם ההחזקה של הנשק בלי מידע על מיקום או דרך אחסון. אין לתייג \"נכון\" אם יש תיאור של אביזרים בלבד, ללא ציון של הנשק עצמו.',\n",
    "            'CIR_AMMU_AMOUNT_WEP': \"האם מופיעים נתונים כמותיים על התחמושת (כמו מספר כדורים,קליעים, מחסניות)? אם מופיע תיאור של מחסנית בלבד ללא פירוט על מספר כדורים, יש להניח שמדובר במחסנית מלאה.יש לתייג נכון גם במקרים בהם הנשק עצמו הוא גם התחמושת(לדוגמא: פגזים),\",\n",
    "            'CIR_PURPOSE' : \"\"\"\n",
    "            האם מופיעה כוונה או מטרה לביצוע העבירה?\n",
    "            תייג 'נכון' אם הטקסט מציין במפורש או במשתמע בבירור את הסיבה לביצוע העבירה, למשל בצע כסף, הגנה עצמית, סכסוך, תדמית, פיגוע ביטחוני וכו'.\n",
    "            תייג 'נכון' גם אם הכוונה מצוינת כהיעדר כוונה או מטרה ספציפית (למשל, עבירה שבוצעה מתוך רשלנות או בטעות).\n",
    "            אין לתייג 'נכון' במקרים הבאים:\n",
    "            - אם הטקסט מתאר רק את המעשה בלי פירוט הסיבה.\n",
    "            - אם מדובר רק על חומרת העבירה, גזר הדין או השלכותיה.\n",
    "            - אם יש דיון בשיקום או נסיבות חיי הנאשם ללא קשר למניע.\n",
    "            - אם ההקשר הוא משפטי בלבד ולא קשור ישירות למניע.\n",
    "            \"\"\",\n",
    "            'GENERAL_CIRCUM': (\n",
    "                \"האם מופיע נסיבות כלליות להקלה על העונש?\"\n",
    "                \"יש לתייג נכון גם אם מופיע אחד מהקריטריונים הבאים באופן משתמע, כל עוד ברור מההקשר שהם מתקיימים :\"\n",
    "                \"(1) הפגיעה האפשרית של העונש בנאשם כולל פגיעה בלתי מידתית בנאשם בשל מאסר ארוך או נסיבותיו האישיות. \"\n",
    "                \"(2) הפגיעה האפשרית של העונש במשפחתו של הנאשם; \"\n",
    "                \"(3) הנזקים שנגרמו לנאשם מביצוע העבירה ומהרשעתו; \"\n",
    "                \"(4) מאמצי הנאשם לשיקום ,שיקום עצמי, חזרה לחיים נורמטיביים , התרשמות שירות המבחן מהליך שיקום, שינוי חיובי,הרתעת ההליך הפלילי לנאשם\"\n",
    "                \"(5) ניסיון הנאשם לתיקון תוצאות העבירה ולפיצוי הנפגעים; \"\n",
    "                \"(6) שיתוף פעולה עם רשויות החוק, כולל אם ההליך הפלילי השפיע על הנאשם באופן מרתיע או משקם. \"\n",
    "                \"(7) התנהגות חיובית של הנאשם ותרומתו לחברה \"\n",
    "                \"(8)  נסיבות חייו של הנאשם טרם ביצוע העבירה, כולל השפעה של גיל, פחד, לחץ חברתי או רגשי על המעשים , מצוקה כלכלית או פגיעה אפשרית בפרנסה. בעיות רפואיות, נפשיות, או חוויות טראומטיות שהובילו לעבירה. השפעת החברה הסובבת, כולל התמכרויות ,הדרדרות לסמים, התדרדרות סביבתית. \"\n",
    "                \"(9) התנהלות חריגה מצד רשויות החוק (עיכובים, עוולות וכו'). \"\n",
    "                \"(10) חלוף זמן משמעותי מאז העבירה והשינוי שחל בנאשם מאז.\"\n",
    "                \"(11)  עברו הפלילי של הנאשם, כולל הרשעות קודמות\"\n",
    "                \"אין לתייג נכון :אזכורים כלליים של חוקי ענישה,סיבות טכניות בלבד כמו חיסכון בזמן שיפוטי.\"\n",
    "                \"אין לתייג נכון אם מוזכר רק המלצת שירות המבחן ללא פירוט נוסף\"\n",
    "            ),\n",
    "            'CIR_STATUS_WEP': \" האם מופיע תיאור של מצב הנשק? תייג 'נכון' אם הטקסט מתאר במפורש את מצב הנשק, כולל: - נשק תקול (לא פועל, מקולקל, לא ירה). - נשק מפורק (מורכב מחלקים, הוסרו ממנו רכיבים). - נשק מופרד מתחמושת (כאשר נשק ותחמושת מוזכרים יחד אך אין אינדיקציה שהם מחוברים). - נשק עם מחסנית בהכנס (מחסנית בתוך הנשק, אך לא בהכרח טעון). - נשק עם כדור בקנה (טעון ומוכן לירי). - נשק תקין (דרוך, פעיל, או מוכן לשימוש).  אם הטקסט מכיל התייחסות לנשק ולתחמושת, יש להניח כברירת מחדל שהם מופרדים, אלא אם נאמר אחרת. אין לתייג נכון אם מדובר במצב התחמושת בלבד ללא הנשק.\",\n",
    "            'REGRET': \"האם מופיעים ביטויים המעידים על חרטה או חוסר חרטה של הנאשם? תייג נכון אם מופיע ביטוי שמעיד על כך שהנאשם התחרט או לא התחרט על מעשיו לדוגמא : הביע חרטה, חרטתו ,לא מתחרט וכו\",\n",
    "            'PUNISHMENT': \"האם מופיע פירוט מדויק של העונש שהוטל בהחלטה הנוכחית בלבד?  תייג 'נכון' רק אם מדובר בעונש שניתן במסגרת פסק הדין הנוכחי ולא עונש שנגזר בתיקים קודמים.  פירוט העונש יכול לכלול:  - מאסר בפועל, כולל חופף/מצטבר  - מאסר על תנאי  - חילוט רכוש  - קנסות ופיצויים  - עבודות שירות  - כל עונש נוסף שנפסק במסגרת ההחלטה הנוכחית בלבד.  אם העונש מתייחס להרשעות קודמות, אין לתייג 'נכון'.\",\n",
    "            'CIR_PLANNING': \"האם מופיעים סימנים המעידים על תכנון מוקדם של העבירה או חוסר בתכנון מוקדם? תייג 'נכון' אם הטקסט כולל אזכור מפורש של תכנון, בין אם הטקסט טוען שהיה תכנון או אם יש הכחשה ישירה של תכנון, צעדים טכניים של הכנה או פעולות שאינן כוללות ראיה ברורה לכך שהנאשם תכנן מראש לבצע עבירה – אינן נחשבות תכנון.\",\n",
    "            'RESPO':'האם מופיעים ביטויים המעידים על קבלת או אי לקיחת אחריות לביצוע העבירה? תייג נכון אם מופיע ביטוי שמעיד על כך שהנאשם לוקח בצורה מלאה או חלקית או לא לוקח אחריות על מעשיו. אין לתייג נכון אם מופיע בטקסט רק הבעת חרטה, צער או הכרה בטעות  ',\n",
    "            'CIR_OBTAIN_WAY_WEP':'.האם מופיע תיאור של הדרך בה הנשק הגיע לידי הנאשם, במפורש או במשתמע? יש לתייג נכון רק אם ניתן להבין כיצד הנשק הגיע אליו מלכתחילה ,אין לתייג נכון אם מדובר רק בהחזקת נשק או בהעברה שלו ממקום למקום ללא הסבר על אופן ההשגה הראשוני- רכישה, ייצור, גניבה וכו׳.... אם מדובר בעסקת נשק, יש לתייג נכון רק אם מצוין שהעסקה הושלמה. אין לתייג נכון אם הטקסט מתאר רק תכנון, ניסיון או הצעה לרכוש נשק, בלי אישור שהנשק התקבל בפועל',\n",
    "            'CIR_USE':'האם מופיע תיאור של שימוש פעיל באמצעי לחימה או חוסר שימוש באמצעי לחימה על ידי הנאשם במהלך העבירה? שימוש פעיל כולל ירי, הפעלת מטען חבלה, הצתת פתיל,השלכת רימון או כל אקט שמטרתו להפעיל אמצעי לחימה . החזקה, נשיאה או טעינת נשק ללא הפעלה אינם נחשבים כשימוש פעיל.'\n",
    "\n",
    "        }\n",
    "        self.output_path = output_path\n",
    "        \n",
    "    def create_zero_shot_prompt(self, text: str, label: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a zero-shot prompt for classification.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input legal text.\n",
    "            label (str): The label to classify.\n",
    "        \n",
    "        Returns:\n",
    "            str: The formatted prompt.\n",
    "        \"\"\"\n",
    "\n",
    "        label_description = self.label_descriptions.get(label, None)\n",
    "    \n",
    "        if label_description is None:\n",
    "            # If label description is not found, skip this label and continue\n",
    "            print(f\"Label '{label}' not found in label_descriptions. Skipping...\")\n",
    "            return None  # Or return a default prompt if needed, depending on your logic\n",
    "\n",
    "        prompt = f\"\"\"קבע אם הטקסט המשפטי כולל את המידע הבא: {label_description}.\n",
    "        התשובה יכולה להיות רק 'נכון' או 'לא נכון'.\n",
    "        ענה אך ורק 'נכון' או 'לא נכון' ללא הסברים נוספים.\n",
    "        \n",
    "    הנחות:\n",
    "    1. אם לא מצוין במפורש אדם אחר, יש להניח שההתייחסות היא לנאשם.\n",
    "    2. אם הטקסט מתייחס לתיק אחר (למשל, \"במקרה של ע\"פ 11/22 הנאשם ירה...\"), אין לתייג אותו כ'נכון' באף מקרה.\n",
    "    3. אם מצוין שם של אדם אחר שאינו הנאשם (למשל, \"פלוני ירה...\"), אין לתייג את הטקסט כ'נכון'.\n",
    "        \"\"\"\n",
    "        prompt += \"---\\nכעת, נתח את הטקסט הבא בהתאם לכללים לעיל.\\n\\n\"\n",
    "        prompt += f\"\"\"טקסט: {text}\n",
    "    תשובה:\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "\n",
    "    def create_few_shot_prompt(self, text: str, label: str, n_examples: int) -> str:\n",
    "        \"\"\"\n",
    "        Generates a few-shot learning prompt using examples.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input legal text.\n",
    "            label (str): The label to classify.\n",
    "            n_examples (int): Number of few-shot examples to include.\n",
    "\n",
    "        Returns:\n",
    "            str: The formatted prompt.\n",
    "        \"\"\"\n",
    "\n",
    "        label_description = self.label_descriptions.get(label, None)\n",
    "        if label_description is None:\n",
    "            print(f\"Label '{label}' not found!\")  # Debugging print\n",
    "            return None\n",
    "\n",
    "        examples = few_shot_examples.get(label, [])[:n_examples]\n",
    "\n",
    "        prompt = f\"\"\"קבע אם הטקסט המשפטי הבא כולל את המידע הבא: {label_description}.\n",
    "        התשובה יכולה להיות רק 'נכון' או 'לא נכון'.\n",
    "        ענה אך ורק 'נכון' או 'לא נכון' ללא הסברים נוספים.\n",
    "        \n",
    "    הנחות:\n",
    "    1. אם לא מצוין במפורש אדם אחר, יש להניח שההתייחסות היא לנאשם.\n",
    "    2. אם הטקסט מתייחס לתיק אחר (למשל, \"במקרה של ת\"פ 111 הנאשם ירה...\"), אין לתייג אותו כ'נכון'.\n",
    "    3. אם מצוין שם של אדם אחר שאינו הנאשם (למשל, \"צאלח ירה...\"), אין לתייג את הטקסט כ'נכון'.\n",
    "        דוגמאות:\n",
    "        \"\"\"\n",
    "\n",
    "        for example in examples:\n",
    "            prompt += f\"\"\"טקסט: {example['text']}\n",
    "            תשובה: {'נכון' if example['label'] == 1 else 'לא נכון'}\n",
    "            \n",
    "            \"\"\"\n",
    "        prompt += \"---\\nכעת, נתח את הטקסט הבא בהתאם לכללים לעיל.\\n\\n\"\n",
    "        prompt += f\"\"\"טקסט: {text}\n",
    "    תשובה:\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def create_cot_prompt(self, text: str, label: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a chain-of-thought (CoT) prompt for reasoning-based classification.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input legal text.\n",
    "            label (str): The label to classify.\n",
    "\n",
    "        Returns:\n",
    "            str: The formatted prompt with step-by-step reasoning.\n",
    "        \"\"\"\n",
    "        label_description = self.label_descriptions.get(label, None)\n",
    "    \n",
    "        if label_description is None:\n",
    "            # If label description is not found, skip this label and continue\n",
    "            print(f\"Label '{label}' not found in label_descriptions. Skipping...\")\n",
    "            return None  # Or return a default prompt if needed, depending on your logic\n",
    "\n",
    "        return f\"\"\"האם הטקסט המשפטי הבא מכיל מידע על {self.label_descriptions[label]}?\n",
    "        \n",
    "        טקסט: {text}\n",
    "        \n",
    "        נתח בקצרה:\n",
    "        1. מהם האלמנטים המרכזיים בטקסט?\n",
    "        2. האם הם קשורים ל-{self.label_descriptions[label]}?\n",
    "        \n",
    "        חשוב: ענה אך ורק 'נכון' או 'לא נכון' בהתבסס על הניתוח לעיל, ללא תוספת הסברים נוספים:\n",
    "        תשובה:\"\"\"\n",
    "    \n",
    "    def create_cot_prompt_long(self, text: str, label: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a detailed and expanded CoT prompt to analyze the given text step by step.\n",
    "        \"\"\"\n",
    "        label_description = self.label_descriptions.get(label, None)\n",
    "    \n",
    "        if label_description is None:\n",
    "            # If label description is not found, skip this label and continue\n",
    "            print(f\"Label '{label}' not found in label_descriptions. Skipping...\")\n",
    "            return None  # Or return a default prompt if needed, depending on your logic\n",
    "\n",
    "        return f\"\"\"בוא ננתח שלב אחר שלב האם הטקסט המשפטי הבא מכיל מידע על {self.label_descriptions[label]}.\n",
    "        \n",
    "        טקסט: {text}\n",
    "        \n",
    "        שלבים לניתוח:\n",
    "        1. מהם האלמנטים המרכזיים המוזכרים בטקסט? \n",
    "        2. האם האלמנטים שזוהו קשורים באופן ישיר או עקיף ל-{self.label_descriptions[label]}? \n",
    "        3. האם יש דיון מפורש או עקיף במשהו הקשור ל-{self.label_descriptions[label]}? \n",
    "        4. בהתבסס על האלמנטים והניתוח, האם ניתן להסיק שהטקסט מכיל מידע רלוונטי לנושא {self.label_descriptions[label]}?\n",
    "\n",
    "        חשוב: ענה אך ורק 'נכון' או 'לא נכון' בהתבסס על הניתוח לעיל, ללא תוספת הסברים נוספים:\n",
    "        תשובה:\"\"\"\n",
    "\n",
    "    def get_prediction(self, prompt: str) -> bool:\n",
    "        \"\"\"\n",
    "        Sends a classification request to OpenAI's model and retrieves the response.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The generated prompt for classification.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if classified as relevant, False otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Call the classifier and get the response\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=8,\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            answer = response.choices[0].message.content.strip().lower()\n",
    "            result= \"נכון\" in answer and \"לא נכון\" not in answer            \n",
    "        ###  Log the prompt and result for debugging\n",
    "            # print('------- PROMPT-------')\n",
    "            # print(prompt)\n",
    "            # print('------- RESPONSE-------')\n",
    "            # print(response)\n",
    "            # print(answer)\n",
    "            # print('------- FINAL-------')\n",
    "            # print(result)\n",
    "            \n",
    "            # Check if the result contains the desired logic\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"error in model: {e}\")\n",
    "            return False\n",
    "\n",
    "    def evaluate_prompting_strategy(self,df: pd.DataFrame,label: str,prompt_type: str = 'zero-shot',n_examples: int = 3) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluates the classifier's performance using a specified prompting strategy.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): A DataFrame containing texts and labels.\n",
    "            label (str): The label to classify.\n",
    "            prompt_type (str, optional): The type of prompting ('zero-shot', 'few-shot', 'chain-of-thought'). Defaults to 'zero-shot'.\n",
    "            n_examples (int, optional): Number of few-shot examples. Defaults to 3.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing accuracy, precision, recall, and F1-score.\n",
    "        \"\"\"\n",
    "        texts = df['text'].tolist()\n",
    "        true_labels = df[label].tolist()\n",
    "        predictions = []\n",
    "        results_data = []\n",
    "        \n",
    "        for text, true_label in tqdm(zip(texts, true_labels),desc=f\"Processing {prompt_type} for {label}\"):\n",
    "            if prompt_type == 'zero-shot':\n",
    "                prompt = self.create_zero_shot_prompt(text, label)\n",
    "            elif prompt_type == 'few-shot':\n",
    "                prompt = self.create_few_shot_prompt(text, label, n_examples)\n",
    "            elif prompt_type == 'chain-of-thought':\n",
    "                prompt = self.create_cot_prompt(text, label)\n",
    "            else: # chain-of-thought-long\n",
    "                prompt = self.create_cot_prompt_long(text, label)\n",
    "                \n",
    "            if not prompt:\n",
    "                continue\n",
    "   \n",
    "            pred = self.get_prediction(prompt)\n",
    "            generated_label = 1 if pred else 0\n",
    "            predictions.append(generated_label)\n",
    "            \n",
    "            # Store result for export\n",
    "            results_data.append({\n",
    "                'text': text,\n",
    "                'true_label': true_label,\n",
    "                'generated_label': generated_label\n",
    "            })\n",
    "            time.sleep(1)  # Rate limiting\n",
    "        \n",
    "        # Save results to CSV\n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        if prompt_type == 'few-shot':\n",
    "            results_df.to_csv(f\"{self.output_path}_wep_{label}_{prompt_type}_{n_examples}.csv\", index=False)\n",
    "        else:\n",
    "            results_df.to_csv(f\"{self.output_path}_wep_{label}_{prompt_type}.csv\", index=False)\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            true_labels, \n",
    "            predictions, \n",
    "            average='binary'\n",
    "        )\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        \n",
    "        return {\n",
    "            'Aaccuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1_Score': f1\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HebrewLegalMultiLabelClassifierDrug:\n",
    "    def __init__(self, output_path: str, model_name: str = \"gpt-4o-mini\"):\n",
    "        self.client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.output_path = output_path\n",
    "        self.model_name = model_name\n",
    "        self.label_descriptions = {\n",
    "             'RESPO':'האחריות על ביצוע העבירה',\n",
    "              'REGRET':'הבעת חרטה',\n",
    "             'CONFESSION': 'האם הנאשם הודה',\n",
    "            'CIR_TYPE': ' סוג הסם המעורב בעבירה',\n",
    "            'CIR_AMOUNT': ' ערכים מספריים עם יחידות מדידה (כגון גרם, ק\"ג, טבליות) המדגישים את הכמות המדויקת של הסם.',\n",
    "            'GENERAL_CIRCUM':\n",
    "                \"האם מופיע נסיבות כלליות להקלה על העונש?\"\n",
    "                \"יש לתייג נכון גם אם מופיע אחד מהקריטריונים הבאים באופן משתמע, כל עוד ברור מההקשר שהם מתקיימים :\"\n",
    "                \"(1) הפגיעה האפשרית של העונש בנאשם כולל פגיעה בלתי מידתית בנאשם בשל מאסר ארוך או נסיבותיו האישיות. \"\n",
    "                \"(2) הפגיעה האפשרית של העונש במשפחתו של הנאשם; \"\n",
    "                \"(3) הנזקים שנגרמו לנאשם מביצוע העבירה ומהרשעתו; \"\n",
    "                \"(4) מאמצי הנאשם לשיקום ,שיקום עצמי, חזרה לחיים נורמטיביים , התרשמות שירות המבחן מהליך שיקום, שינוי חיובי,הרתעת ההליך הפלילי לנאשם\"\n",
    "                \"(5) ניסיון הנאשם לתיקון תוצאות העבירה ולפיצוי הנפגעים; \"\n",
    "                \"(6) שיתוף פעולה עם רשויות החוק, כולל אם ההליך הפלילי השפיע על הנאשם באופן מרתיע או משקם. \"\n",
    "                \"(7) התנהגות חיובית של הנאשם ותרומתו לחברה \"\n",
    "                \"(8)  נסיבות חייו של הנאשם טרם ביצוע העבירה, כולל השפעה של גיל, פחד, לחץ חברתי או רגשי על המעשים , מצוקה כלכלית או פגיעה אפשרית בפרנסה. בעיות רפואיות, נפשיות, או חוויות טראומטיות שהובילו לעבירה. השפעת החברה הסובבת, כולל התמכרויות ,הדרדרות לסמים, התדרדרות סביבתית. \"\n",
    "                \"(9) התנהלות חריגה מצד רשויות החוק (עיכובים, עוולות וכו'). \"\n",
    "                \"(10) חלוף זמן משמעותי מאז העבירה והשינוי שחל בנאשם מאז.\"\n",
    "                \"(11)  עברו הפלילי של הנאשם, כולל הרשעות קודמות\"\n",
    "                \"אין לתייג נכון :אזכורים כלליים של חוקי ענישה,סיבות טכניות בלבד כמו חיסכון בזמן שיפוטי.\"\n",
    "                \"אין לתייג נכון אם מוזכר רק המלצת שירות המבחן ללא פירוט נוסף\"\n",
    "            ,\n",
    "            'PUNISHMENT': \"האם מופיע פירוט של פירוט מדויק של העונש שהוטל בהחלטה הנוכחית בלבד, כולל פרטים כמו סוג העונש (למשל, מאסר בפועל, מאסר על תנאי, קנס כספי, שלילת רישיון, צו מבחן, חילוט רכוש, השמדת סמים,  או שירות לתועלת הציבור) ומשך הזמן, תוך התמקדות בעונש הנוכחי בלבד ולא בהתייחסות לעונשים קודמים או להמלצות עתידיות.?\",\n",
    "            'CIR_ROLE':\n",
    "            \"\"\"\n",
    "יש לתייג נכון אם אחד מהקריטריונים הבאים מתקיים, גם אם הוא משתמע מההקשר:\n",
    "(1)\tלנאשם היה תפקיד מרכזי בביצוע העבירה – יוזמה, תכנון, הובלה או מימון הפעולה.\n",
    "(2)\tהנאשם ביצע את העבירה כמו יבוא , הברחת סמים, גידול, אחזקה, או הפצה.\n",
    "(3)\tהנאשם שימש כמתווך – כלומר, סייע בהעברת הסם בין צדדים שונים או היה מעורב בהפצה.\n",
    "(4)\tהנאשם סייע באופן פעיל בביצוע העבירה – למשל, סיפק ציוד, מקום, או תמיכה לוגיסטית.\n",
    "(5)\tהנאשם שימש כבלדר או שליח להעברת סמים, בין אם ידע מראש את מטרת הפעולה ובין אם נוצל על ידי אחרים.\n",
    "(6)\tהתיאור מבהיר את מעמדו של הנאשם בעבירה, למשל אם היה הדמות המרכזית או דמות שולית.\n",
    "(7)\tיש אזכור לתכנון, שיטתיות, או קישור לגורמים נוספים שמצביעים על מעורבות מהותית בעבירה.\n",
    "(8)\tהנאשם מכר או סיפק סם לאחרים, בין אם תמורת כסף ובין אם לא.\n",
    "אין לתייג נכון אם:\n",
    "הטקסט אינו מפרט את תפקידו של הנאשם או את רמת מעורבותו בעבירה.\n",
    "(1)יש אזכור להחזקת סם בלבד ללא תיאור של ייבוא, מכירה, תיווך או הפצה.\n",
    "(2)ההתייחסות היא לנסיבות חיצוניות כמו מצבו האישי של הנאשם, השלכות העבירה עליו, או רקע כללי ללא מידע על תפקידו בעבירה.\n",
    "(3)מוזכר רק ההליך המשפטי או החוקי (למשל, התייחסות לעונש, הרשעה או פסיקה) ללא פירוט על תפקיד הנאשם.\n",
    "(4)אין עדות לפעולה יזומה מצד הנאשם (למשל, אין אזכור למכירה, תיווך, גידול, ייבוא או הברחה).\n",
    "\n",
    "            \"\"\"\n",
    "            ,\n",
    "            'CIR_EQ': \"\"\"\n",
    "            מופיע אזכור למעבדה או לתשתית המשמשת לגידול, הפקה או הכנה של סמים. קיימת התייחסות ישירה למונח \"מעבדה\" או לתיאור ציוד המשמש לייצור סמים. ציוד כזה כולל אמצעים כגון גופי תאורה ייעודיים, מערכות אוורור והשקיה, אדניות, חומרי דישון, משקלים, שנאים, מפוחים, מסננים, נורות חימום, מזגנים, או כל רכיב טכני אחר שמטרתו לתמוך בתהליך הגידול והייצור של סמים.\n",
    "בנוסף, זה חל גם על תיאורים של תכנון מוקדם להקמת מעבדה, שכירת מקום ייעודי לשם כך, רכישת ציוד בהיקף משמעותי, או ביצוע פעולות כמו חיבור פיראטי לחשמל לצורך תפעול המקום. כאשר מדובר במקרה שבו הוזכרה מעבדה במפורש, הדבר מהווה קריטריון חד-משמעי לסיווג לליבל , במקרים שבהם המונח \"מעבדה\" אינו מופיע, אך ישנו פירוט של ציוד מקצועי או מערכת מתקדמת לגידול והפקת סמים, עדיין ניתן לשייך את הקטע ללייבל זה.\n",
    "\n",
    "            \"\"\"\n",
    "        }\n",
    "        self.output_path = output_path\n",
    "\n",
    "    def create_zero_shot_prompt(self, text: str, label: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a zero-shot prompt for classification.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input legal text.\n",
    "            label (str): The label to classify.\n",
    "\n",
    "        Returns:\n",
    "            str: The formatted prompt.\n",
    "        \"\"\"\n",
    "\n",
    "        label_description = self.label_descriptions.get(label, None)\n",
    "\n",
    "        if label_description is None:\n",
    "            # If label description is not found, skip this label and continue\n",
    "            print(f\"Label '{label}' not found in label_descriptions. Skipping...\")\n",
    "            return None  # Or return a default prompt if needed, depending on your logic\n",
    "\n",
    "        prompt = f\"\"\"קבע אם הטקסט המשפטי כולל את המידע הבא: {label_description}.\n",
    "        התשובה יכולה להיות רק 'נכון' או 'לא נכון'.\n",
    "        ענה אך ורק 'נכון' או 'לא נכון' ללא הסברים נוספים.\n",
    "\n",
    "    הנחות:\n",
    "    1. אם לא מצוין במפורש אדם אחר, יש להניח שההתייחסות היא לנאשם.\n",
    "    2. אם הטקסט מתייחס לתיק אחר (למשל, \"במקרה של ע\"פ 11/22 הנאשם ירה...\"), אין לתייג אותו כ'נכון' באף מקרה.\n",
    "    3. אם מצוין שם של אדם אחר שאינו הנאשם (למשל, \"פלוני ירה...\"), אין לתייג את הטקסט כ'נכון'.\n",
    "        \"\"\"\n",
    "        prompt += \"---\\nכעת, נתח את הטקסט הבא בהתאם לכללים לעיל.\\n\\n\"\n",
    "        prompt += f\"\"\"טקסט: {text}\n",
    "    תשובה:\"\"\"\n",
    "\n",
    "        return prompt\n",
    "    def create_few_shot_prompt(self, text: str, label: str, n_examples: int) -> str:\n",
    "        \"\"\"\n",
    "        Generates a few-shot learning prompt using examples.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input legal text.\n",
    "            label (str): The label to classify.\n",
    "            n_examples (int): Number of few-shot examples to include.\n",
    "\n",
    "        Returns:\n",
    "            str: The formatted prompt.\n",
    "        \"\"\"\n",
    "\n",
    "        label_description = self.label_descriptions.get(label, None)\n",
    "        if label_description is None:\n",
    "            print(f\"Label '{label}' not found!\")  # Debugging print\n",
    "            return None\n",
    "\n",
    "        examples = few_shot_examples.get(label, [])[:n_examples]\n",
    "\n",
    "        prompt = f\"\"\"קבע אם הטקסט המשפטי הבא כולל את המידע הבא: {label_description}.\n",
    "        התשובה יכולה להיות רק 'נכון' או 'לא נכון'.\n",
    "        ענה אך ורק 'נכון' או 'לא נכון' ללא הסברים נוספים.\n",
    "\n",
    "    הנחות:\n",
    "    1. אם לא מצוין במפורש אדם אחר, יש להניח שההתייחסות היא לנאשם.\n",
    "    2. אם הטקסט מתייחס לתיק אחר (למשל, \"במקרה של ת\"פ 111 הנאשם ירה...\"), אין לתייג אותו כ'נכון'.\n",
    "    3. אם מצוין שם של אדם אחר שאינו הנאשם (למשל, \"צאלח ירה...\"), אין לתייג את הטקסט כ'נכון'.\n",
    "        דוגמאות:\n",
    "        \"\"\"\n",
    "\n",
    "        for example in examples:\n",
    "            prompt += f\"\"\"טקסט: {example['text']}\n",
    "            תשובה: {'נכון' if example['label'] == 1 else 'לא נכון'}\n",
    "\n",
    "            \"\"\"\n",
    "        prompt += \"---\\nכעת, נתח את הטקסט הבא בהתאם לכללים לעיל.\\n\\n\"\n",
    "        prompt += f\"\"\"טקסט: {text}\n",
    "    תשובה:\"\"\"\n",
    "\n",
    "        return prompt\n",
    "    def create_cot_prompt(self, text: str, label: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a concise CoT prompt to analyze the given text with minimal reasoning.\n",
    "        \"\"\"\n",
    "        label_description = self.label_descriptions.get(label, None)\n",
    "\n",
    "        if label_description is None:\n",
    "            # If label description is not found, skip this label and continue\n",
    "            print(f\"Label '{label}' not found in label_descriptions. Skipping...\")\n",
    "            return None  # Or return a default prompt if needed, depending on your logic\n",
    "\n",
    "        return f\"\"\"האם הטקסט המשפטי הבא מכיל מידע על {self.label_descriptions[label]}?\n",
    "\n",
    "        טקסט: {text}\n",
    "\n",
    "        נתח בקצרה:\n",
    "        1. מהם האלמנטים המרכזיים בטקסט?\n",
    "        2. האם הם קשורים ל-{self.label_descriptions[label]}?\n",
    "\n",
    "        חשוב: ענה אך ורק 'נכון' או 'לא נכון' בהתבסס על הניתוח לעיל, ללא תוספת הסברים נוספים:\n",
    "        תשובה:\"\"\"\n",
    "\n",
    "    def create_cot_prompt_long(self, text: str, label: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a detailed and expanded CoT prompt to analyze the given text step by step.\n",
    "        \"\"\"\n",
    "        label_description = self.label_descriptions.get(label, None)\n",
    "\n",
    "        if label_description is None:\n",
    "            # If label description is not found, skip this label and continue\n",
    "            print(f\"Label '{label}' not found in label_descriptions. Skipping...\")\n",
    "            return None  # Or return a default prompt if needed, depending on your logic\n",
    "\n",
    "        return f\"\"\"בוא ננתח שלב אחר שלב האם הטקסט המשפטי הבא מכיל מידע על {self.label_descriptions[label]}.\n",
    "\n",
    "        טקסט: {text}\n",
    "\n",
    "        שלבים לניתוח:\n",
    "        1. מהם האלמנטים המרכזיים המוזכרים בטקסט?\n",
    "        2. האם האלמנטים שזוהו קשורים באופן ישיר או עקיף ל-{self.label_descriptions[label]}?\n",
    "        3. האם יש דיון מפורש או עקיף במשהו הקשור ל-{self.label_descriptions[label]}?\n",
    "        4. בהתבסס על האלמנטים והניתוח, האם ניתן להסיק שהטקסט מכיל מידע רלוונטי לנושא {self.label_descriptions[label]}?\n",
    "\n",
    "        חשוב: ענה אך ורק 'נכון' או 'לא נכון' בהתבסס על הניתוח לעיל, ללא תוספת הסברים נוספים:\n",
    "        תשובה:\"\"\"\n",
    "\n",
    "    def get_prediction(self, prompt: str) -> bool:\n",
    "        try:\n",
    "            # Call the classifier and get the response\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=5,\n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            answer = response.choices[0].message.content.strip().lower()\n",
    "            result= \"נכון\" in answer and \"לא נכון\" not in answer\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"error in model: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    def evaluate_prompting_strategy(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        label: str,\n",
    "        prompt_type: str = 'zero-shot',\n",
    "        n_examples: int = 3\n",
    "    ) -> Dict[str, float]:\n",
    "        texts = df['text'].tolist()\n",
    "        true_labels = df[label].tolist()\n",
    "        predictions = []\n",
    "        results_data = []\n",
    "\n",
    "\n",
    "\n",
    "        for text, true_label in tqdm(zip(texts, true_labels),desc=f\"Processing {prompt_type} for {label}\"):\n",
    "            if prompt_type == 'zero-shot':\n",
    "                prompt = self.create_zero_shot_prompt(text, label)\n",
    "            elif prompt_type == 'few-shot':\n",
    "                prompt = self.create_few_shot_prompt(text, label, n_examples)\n",
    "            elif prompt_type == 'chain-of-thought':\n",
    "                prompt = self.create_cot_prompt(text, label)\n",
    "            else: # chain-of-thought-long\n",
    "                prompt = self.create_cot_prompt_long(text, label)\n",
    "\n",
    "            pred = self.get_prediction(prompt)\n",
    "            generated_label = 1 if pred else 0\n",
    "            predictions.append(generated_label)\n",
    "\n",
    "            # Store result for export\n",
    "            results_data.append({\n",
    "                'text': text,\n",
    "                'true_label': true_label,\n",
    "                'generated_label': generated_label\n",
    "            })\n",
    "            time.sleep(1)  # Rate limiting\n",
    "\n",
    "        # Save results to CSV\n",
    "        results_df = pd.DataFrame(results_data)\n",
    "        if prompt_type == 'few-shot':\n",
    "            results_df.to_csv(f\"{self.output_path}_drug_{label}_{prompt_type}_{n_examples}.csv\", index=False)\n",
    "        else:\n",
    "            results_df.to_csv(f\"{self.output_path}_drug_{label}_{prompt_type}.csv\", index=False)\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            true_labels,\n",
    "            predictions,\n",
    "            average='binary'\n",
    "        )\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "        return {\n",
    "            'Aaccuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1_Score': f1\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"api-key\" \n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "data_file = \"/Data/tagged_data_manualy/weapon/test.csv\"\n",
    "output_directory = \"/Data/tagged_data_manualy/weapon/results/\" \n",
    "Drug_or_Wep = \"Wep\"  # Change here wich classifier to use; Drug or Wep\n",
    "example_counts = [3] # Number of examples per label to use in the few-shot experiments\n",
    "labels=[\"CONFESSION\",\"CIR_TYPE_WEP\",\"CIR_HELD_WAY_WEP\",\"CIR_AMMU_AMOUNT_WEP\",\"CIR_PURPOSE\",\"GENERAL_CIRCUM\",\"CIR_STATUS_WEP\",\"REGRET\",\"PUNISHMENT\",\"CIR_PLANNING\",\"RESPO\",\"CIR_OBTAIN_WAY_WEP\",\"CIR_USE\"]\n",
    "CSV_LOG_FILE = \"batch_log.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multilabel_experiments(data_file: str,classifier=HebrewLegalMultiLabelClassifierWep(\"\"),labels_to_test: List[str] = None,n_examples: int = 3,prompt_types: List[str] = ['zero-shot', 'few-shot', 'chain-of-thought','chain-of-thought-long']) -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Runs multi-label classification experiments on legal text data using different prompting strategies.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_file : str\n",
    "        Path to the CSV file containing the dataset.\n",
    "    classifier : HebrewLegalMultiLabelClassifierWep or HebrewLegalMultiLabelClassifierDrug\n",
    "        The classifier instance responsible for evaluating the text classification.\n",
    "    labels_to_test : List[str], optional\n",
    "        List of labels (column names) to evaluate. If None, all suitable labels from the dataset will be used.\n",
    "    n_examples : int, default=3\n",
    "        Number of few-shot examples to include in the prompt.\n",
    "    prompt_types : List[str], default=['zero-shot', 'few-shot', 'chain-of-thought', 'chain-of-thought-long']\n",
    "        List of different prompt types to use for classification.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : Dict[str, Dict[str, Dict[str, float]]]\n",
    "        A nested dictionary containing classification results per label\n",
    "    \"\"\"   \n",
    "    results = {}\n",
    "    \n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(data_file)\n",
    "    # Keep only relevant columns for text and label\n",
    "    if labels_to_test is not None:\n",
    "        df = df[['text'] + labels_to_test]  # Keep 'text' and the labels to test\n",
    "\n",
    "    # Drop rows where 'text' or any label in 'labels_to_test' is NaN\n",
    "    df = df.dropna(subset=['text'] + labels_to_test)\n",
    "\n",
    "    print(df.head())\n",
    "    # If no labels are specified, default to all columns except 'text'\n",
    "    if labels_to_test is None:\n",
    "        labels_to_test = [col for col in df.columns if col != 'text' and df[col].dtype in ['int64', 'bool']]\n",
    "        \n",
    "    print(labels_to_test)\n",
    "    \n",
    "    for label in labels_to_test:\n",
    "        if label == 'reject' or label == 'verdict':\n",
    "            continue\n",
    "        print(f\"\\nProcessing label: {label}\")\n",
    "        label_results = {}\n",
    "        \n",
    "        for prompt_type in prompt_types:\n",
    "            print(f\"Running evaluation for {prompt_type} with {n_examples} examples...\")\n",
    "            # generate_jsonl_file(df, label,prompt_type,n_examples,classifier)\n",
    "            label_results[f\"{prompt_type} - {n_examples} examples\"] = classifier.evaluate_prompting_strategy(\n",
    "                df,\n",
    "                label,\n",
    "                prompt_type,\n",
    "                n_examples=n_examples\n",
    "            )\n",
    "\n",
    "        results[label] = label_results\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main execution script for running multi-label classification experiments.\n",
    "\n",
    "    Steps:\n",
    "    1. Creates the output directory if it does not exist.\n",
    "    2. Initializes the classifier based on the category (weapons or drugs).\n",
    "    3. Iterates over different numbers of few-shot examples.\n",
    "    4. Sets the appropriate prompt types for each experiment.\n",
    "    5. Runs the classification experiments for each label.\n",
    "    6. Prints the classification results, including performance metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    if Drug_or_Wep == \"Wep\":\n",
    "        classifier = HebrewLegalMultiLabelClassifierWep(output_path=output_directory)\n",
    "    else:\n",
    "        classifier = HebrewLegalMultiLabelClassifierDrug(output_path=output_directory)\n",
    "    \n",
    "    \n",
    "    # Iterate over the number of examples and run experiments\n",
    "    for n_samples in example_counts:\n",
    "        print(f\"\\nRunning experiments with {n_samples} examples per label...\\n\")\n",
    "        \n",
    "        # Set prompt types conditionally\n",
    "        if n_samples == 1:\n",
    "            prompt_types = ['zero-shot', 'few-shot', 'chain-of-thought', 'chain-of-thought-long']\n",
    "        else:\n",
    "            prompt_types = ['few-shot']\n",
    "        \n",
    "        # Run the experiment with the specified prompt types\n",
    "        results = run_multilabel_experiments(\n",
    "            data_file=data_file,\n",
    "            n_examples=n_samples,\n",
    "            prompt_types=prompt_types,  # Pass the prompt types\n",
    "            classifier = classifier,\n",
    "            labels_to_test=labels\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Results for {n_samples} examples per label:\")\n",
    "        for label, strategies in results.items():\n",
    "            print(f\"\\nResults for label: {label}\")\n",
    "            for strategy, metrics in strategies.items():\n",
    "                print(f\"\\nStrategy: {strategy}\")\n",
    "                for metric, value in metrics.items():\n",
    "                    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create batch file\n",
    "def generate_jsonl_file(output_directory,df, label,prompt_type,n_examples,classifier,Drug_or_Wep):\n",
    "    \"\"\"\n",
    "    Generates a JSONL batch file for classification requests and saves metadata for tracking.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    output_directory : str\n",
    "        Path to the directory where batch files will be saved.\n",
    "    df : pd.DataFrame\n",
    "        Dataframe containing the input text and labels.\n",
    "    label : str\n",
    "        The label being classified.\n",
    "    prompt_type : str\n",
    "        The type of prompting strategy (e.g., 'zero-shot', 'few-shot', 'chain-of-thought').\n",
    "    n_examples : int\n",
    "        Number of few-shot examples to include in the prompt.\n",
    "    classifier : object\n",
    "        The classifier instance used to generate prompts.\n",
    "    Drug_or_Wep : str\n",
    "        Indicates whether the classification is for \"drugs\" or \"weapons\".\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The path to the batch file directory.\n",
    "    \"\"\"\n",
    "    batch_file_dir= os.path.join(output_directory,f\"batch_files_{Drug_or_Wep}\")\n",
    "    os.makedirs(batch_file_dir, exist_ok=True)\n",
    "    batch_file_path = os.path.join(batch_file_dir,f\"batch_{label}_{prompt_type}.jsonl\")\n",
    "    metadata_file_path = batch_file_path.replace(\".jsonl\", \"_metadata.json\")\n",
    "\n",
    "    texts = df['text'].tolist()\n",
    "    true_labels = df[label].tolist()\n",
    "    metadata = {}  # Store mapping of custom_id → (text, true_label)\n",
    "\n",
    "    with open(batch_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for text, true_label in tqdm(zip(texts, true_labels),desc=f\"Processing {prompt_type} for {label}\"):\n",
    "            if prompt_type == 'zero-shot':\n",
    "                prompt = classifier.create_zero_shot_prompt(text, label)\n",
    "            elif prompt_type == 'few-shot':\n",
    "                prompt = classifier.create_few_shot_prompt(text, label, n_examples)\n",
    "            elif prompt_type == 'chain-of-thought':\n",
    "                prompt = classifier.create_cot_prompt(text, label)\n",
    "            else: # chain-of-thought-long\n",
    "                prompt = classifier.create_cot_prompt_long(text, label)\n",
    "                \n",
    "            if not prompt:\n",
    "                continue\n",
    "\n",
    "            # Unique ID for tracking responses\n",
    "            custom_id = str(uuid.uuid4())\n",
    "\n",
    "            # Store metadata in a separate file\n",
    "            metadata[custom_id] = {\"text\": text, \"true_label\": true_label}\n",
    "            if label==\"GENERAL_CIRCUM\" :\n",
    "                request = {\n",
    "                \"custom_id\": custom_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"model\": \"gpt-4o\",\n",
    "                    \"max_tokens\": 8,\n",
    "                    \"temperature\": 0.0\n",
    "                    }\n",
    "            }\n",
    "            else:\n",
    "                request = {\n",
    "                \"custom_id\": custom_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"model\": \"gpt-4o-mini\",\n",
    "                    \"max_tokens\": 8,\n",
    "                    \"temperature\": 0.0\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(request) + \"\\n\")\n",
    "\n",
    "    # Save metadata separately\n",
    "    with open(metadata_file_path, \"w\", encoding=\"utf-8\") as meta_f:\n",
    "        json.dump(metadata, meta_f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return batch_file_dir\n",
    "\n",
    "def run_multilabel_experiments_batches(output_directory:str,Drug_or_Wep:str,data_file: str,classifier=HebrewLegalMultiLabelClassifierWep(\"\"),labels_to_test: List[str] = None,n_examples: int = 3,prompt_types: List[str] = ['zero-shot', 'few-shot', 'chain-of-thought','chain-of-thought-long']) -> Dict[str, Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Runs batch classification experiments and generates batch files for each label and prompt type.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(data_file)\n",
    "    # Keep only relevant columns for text and label\n",
    "\n",
    "    if labels_to_test is not None:\n",
    "        df = df[['text'] + labels_to_test]  # Keep 'text' and the labels to test\n",
    "\n",
    "    # Drop rows where 'text' or any label in 'labels_to_test' is NaN\n",
    "    df = df.dropna(subset=['text'] + labels_to_test)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    print(df.head())\n",
    "    # If no labels are specified, default to all columns except 'text'\n",
    "    if labels_to_test is None:\n",
    "        labels_to_test = [col for col in df.columns if col != 'text' and df[col].dtype in ['int64', 'bool']]\n",
    "        \n",
    "    print(labels_to_test)\n",
    "    \n",
    "    for label in labels_to_test:\n",
    "        if label == 'reject' or label == 'verdict':\n",
    "            continue\n",
    "        print(f\"\\nProcessing label: {label}\")\n",
    "        \n",
    "        for prompt_type in prompt_types:\n",
    "            print(f\"genrate batch file for {prompt_type} with {n_examples} examples...\")\n",
    "            batch_file_dir=generate_jsonl_file(output_directory,df, label,prompt_type,n_examples,classifier,Drug_or_Wep)\n",
    "    return batch_file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to upload a file to OpenAI\n",
    "def upload_file(client,file_path):\n",
    "    \"\"\"\n",
    "    Uploads a file to OpenAI for batch processing.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        response = client.files.create(\n",
    "            file=f,\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "    return response.id  # Returns the uploaded file ID\n",
    "\n",
    "# Function to submit batch files using uploaded file ID\n",
    "def submit_batch(client,input_file_id):\n",
    "    \"\"\"\n",
    "    Submits a batch job to OpenAI.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.batches.create(\n",
    "            input_file_id=input_file_id,  # Correct parameter name\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window= \"24h\"\n",
    "        )\n",
    "        print(f\"Batch submitted successfully! Batch ID: {response.id}\")\n",
    "        return response.id  # Returns the batch job ID\n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"Error submitting batch: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def download_batch_results(client, batch_status, batch_file, batch_dir):\n",
    "    \"\"\"\n",
    "    Downloads batch results from OpenAI.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    client : OpenAI client object\n",
    "        The API client for OpenAI.\n",
    "    batch_status : object\n",
    "        The status object of the batch job.\n",
    "    batch_file : str\n",
    "        The batch file name.\n",
    "    batch_dir : str\n",
    "        Directory where results will be stored.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the saved results file.\n",
    "    pd.DataFrame\n",
    "        DataFrame containing processed batch results.\n",
    "    \"\"\"\n",
    "\n",
    "    if batch_status.status == \"completed\":\n",
    "        output_file_path = os.path.join(batch_dir, f\"results_{batch_file.replace('.jsonl', '.json')}\")\n",
    "        output_csv_path = os.path.join(batch_dir, f\"results_{batch_file.replace('.jsonl', '.csv')}\")\n",
    "        metadata_file_path = os.path.join(batch_dir, f\"{batch_file.replace('.jsonl', '_metadata.json')}\")\n",
    "\n",
    "        # Retrieve file content\n",
    "        response = client.files.content(batch_status.output_file_id)\n",
    "\n",
    "        # Read content properly\n",
    "        file_content = response.read()  # Ensure we get bytes\n",
    "\n",
    "        with open(output_file_path, \"wb\") as f:\n",
    "            f.write(file_content)  # Write the binary content\n",
    "        \n",
    "        print(f\"Results saved to: {output_file_path}\")\n",
    "\n",
    "        df_results = process_batch_results(output_file_path, metadata_file_path,output_csv_path)\n",
    "\n",
    "\n",
    "        return output_file_path,df_results\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def process_batch_results(results_file, metadata_file, output_csv):\n",
    "    \"\"\"\n",
    "    Processes batch results and saves them as a CSV file.\n",
    "    \"\"\"\n",
    "    # Check if metadata file exists\n",
    "    if not os.path.exists(metadata_file):\n",
    "        print(f\"Metadata file not found: {metadata_file}\")\n",
    "        return None\n",
    "\n",
    "    # Load metadata\n",
    "    with open(metadata_file, \"r\", encoding=\"utf-8\") as meta_f:\n",
    "        try:\n",
    "            metadata = json.load(meta_f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error: Metadata file is not valid JSON.\")\n",
    "            return None\n",
    "\n",
    "    # Load results and add GPT response to each record\n",
    "    results = []\n",
    "    with open(results_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                response_data = json.loads(line)\n",
    "                custom_id = response_data.get(\"custom_id\")\n",
    "                answer = response_data.get(\"response\", {})\\\n",
    "                                      .get(\"body\", {})\\\n",
    "                                      .get(\"choices\", [{}])[0]\\\n",
    "                                      .get(\"message\", {})\\\n",
    "                                      .get(\"content\", \"\").strip()\n",
    "\n",
    "                # Compute generated label: \"נכון\" must be present and \"לא נכון\" must NOT be present\n",
    "                generated_label_ = \"נכון\" in answer and \"לא נכון\" not in answer\n",
    "                generated_label = 1 if generated_label_ else 0\n",
    "\n",
    "                # Match with metadata\n",
    "                if custom_id in metadata:\n",
    "                    text = metadata[custom_id][\"text\"]\n",
    "                    true_label = metadata[custom_id][\"true_label\"]\n",
    "                    # Append full GPT response as well as computed labels\n",
    "                    results.append({\n",
    "                        \"text\": text,\n",
    "                        \"true_label\": true_label,\n",
    "                        \"generated_label\": generated_label,\n",
    "                        \"gpt_response\": answer\n",
    "                    })\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping malformed JSON line: {line}\")\n",
    "\n",
    "    if not results:\n",
    "        print(\"No valid results found in batch output.\")\n",
    "        return None\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"CSV saved to: {output_csv}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def log_batch_completion(batch_file, batch_id,CSV_LOG_FILE):\n",
    "    \"\"\"Logs completed batches in a CSV file.\"\"\"\n",
    "    file_exists = os.path.exists(CSV_LOG_FILE)\n",
    "    with open(CSV_LOG_FILE, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"batch_file\", \"batch_id\"])  # Write header if file doesn't exist\n",
    "        writer.writerow([batch_file, batch_id])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main execution script for batch processing:\n",
    "    1. Creates the output directory if it doesn't exist.\n",
    "    2. Initializes the classifier based on the category (weapons or drugs).\n",
    "    3. Generates batch files for each label and prompt type.\n",
    "    4. Uploads the batch files to OpenAI for processing.\n",
    "    5. Submits the uploaded files as batch jobs.\n",
    "    6. Logs submitted batch files.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    if Drug_or_Wep == \"Wep\":\n",
    "        classifier = HebrewLegalMultiLabelClassifierWep(output_path=output_directory)\n",
    "    else:\n",
    "        classifier = HebrewLegalMultiLabelClassifierDrug(output_path=output_directory)\n",
    "\n",
    "    # Step 1: Create all batch files\n",
    "    for n_samples in example_counts:\n",
    "        print(f\"\\n🔹 Running experiments with {n_samples} examples per label...\\n\")\n",
    "        \n",
    "        prompt_types = ['few-shot'] if n_samples != 1 else ['zero-shot', 'few-shot', 'chain-of-thought', 'chain-of-thought-long']\n",
    "        # prompt_types=['zero-shot']\n",
    "        batch_dir = run_multilabel_experiments_batches(\n",
    "            output_directory=output_directory,\n",
    "            data_file=data_file,\n",
    "            n_examples=n_samples,\n",
    "            prompt_types=prompt_types,  \n",
    "            classifier=classifier,\n",
    "            labels_to_test=labels,\n",
    "            Drug_or_Wep=Drug_or_Wep\n",
    "        )\n",
    "    \n",
    "    # Step 2: List batch files for each label\n",
    "    batch_files = []\n",
    "    for label in labels:\n",
    "        label_batch_dir = os.path.join(output_directory, f\"batch_files_{Drug_or_Wep}\")\n",
    "        label_files = [f for f in os.listdir(label_batch_dir) if f.startswith(f\"batch_{label}_\") and f.endswith(\".jsonl\")]\n",
    "        batch_files.extend(label_files)\n",
    "\n",
    "    batch_jobs = {}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_file in batch_files:\n",
    "        batch_path = os.path.join(batch_dir, batch_file)\n",
    "        file_id = upload_file(client, batch_path)\n",
    "        print(f\"✅ Uploaded {batch_file}, File ID: {file_id}\")\n",
    "        \n",
    "        batch_id = submit_batch(client, file_id)\n",
    "        batch_jobs[batch_file] = batch_id\n",
    "        print(f\"🚀 Submitted {batch_file} with Batch ID: {batch_id}\")\n",
    "        \n",
    "        # Start a new batch every 30 sec\n",
    "        while time.time() - start_time < 30:\n",
    "            time.sleep(5)  # Small wait to avoid busy looping\n",
    "        start_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Monitors the status of submitted batch jobs and handles retries for failed or cancelled jobs.\n",
    "    \n",
    "    Steps:\n",
    "    1. Retrieve the status of each active batch from OpenAI.\n",
    "    2. If a batch is completed, log the completion and download the results.\n",
    "    3. If a batch fails, re-upload the corresponding file and resubmit the batch.\n",
    "    4. If a batch is cancelled, re-upload and resubmit the batch.\n",
    "    5. If a batch is still in progress, print the current status, including total, completed, and failed requests.\n",
    "    6. Attempt to retrieve token usage statistics if available.\n",
    "    7. Wait for 2 minutes before checking batch statuses again.\n",
    "\"\"\"\n",
    "while batch_jobs:\n",
    "    for batch_file, batch_id in list(batch_jobs.items()):\n",
    "        batch_status = client.batches.retrieve(batch_id)\n",
    "        if batch_status.status == \"completed\":\n",
    "            print(f\"Batch {batch_id} ({batch_file}) completed!\")\n",
    "            log_batch_completion(batch_file, batch_id,CSV_LOG_FILE)\n",
    "            download_batch_results(client, batch_status, batch_file, batch_dir)\n",
    "            del batch_jobs[batch_file]  # Remove completed batch from tracking\n",
    "        elif batch_status.status == \"failed\":\n",
    "            print(f\" Batch {batch_id} ({batch_file}) failed! Restarting...\")\n",
    "            file_id = upload_file(client, os.path.join(batch_dir, batch_file))\n",
    "            new_batch_id = submit_batch(client, file_id)\n",
    "            batch_jobs[batch_file] = new_batch_id  # Replace failed batch with new one\n",
    "            print(f\" Restarted {batch_file} with new Batch ID: {new_batch_id}\")\n",
    "        elif batch_status.status == \"cancelled\":\n",
    "            print(f\"⚠️ Batch {batch_id} ({batch_file}) was cancelled! Restarting...\")\n",
    "            file_id = upload_file(client, os.path.join(batch_dir, batch_file))\n",
    "            new_batch_id = submit_batch(client, file_id)\n",
    "            batch_jobs[batch_file] = new_batch_id\n",
    "            print(f\"Restarted {batch_file} with new Batch ID: {new_batch_id}\")\n",
    "        elif batch_status.status == \"in_progress\":\n",
    "            # Print batch status and request counts\n",
    "            print(f\"Batch ID: {batch_status.id}\")\n",
    "            print(f\"Batch file: {batch_file}\")\n",
    "            print(f\"Status: {batch_status.status}\")\n",
    "            print(f\"Total Requests: {batch_status.request_counts.total}\")\n",
    "            print(f\"Completed Requests: {batch_status.request_counts.completed}\")\n",
    "            print(f\"Failed Requests: {batch_status.request_counts.failed}\")\n",
    "            print(\"__________________________\")\n",
    "\n",
    "    print(f\" Checking batch statuses again in 2 minutes...\")\n",
    "    time.sleep(120)\n",
    "\n",
    "print(\"\\n All batches have been processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(output_directory, labels, Drug_or_Wep):\n",
    "    \"\"\"\n",
    "    Evaluates model performance for each label based on classification metrics.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    output_directory : str\n",
    "        Path to the output directory where batch results are stored.\n",
    "    labels : list\n",
    "        List of classification labels to evaluate.\n",
    "    Drug_or_Wep : str\n",
    "        Specifies if the evaluation is for \"drugs\" or \"weapons\".\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing precision, recall, F1-score, and accuracy for each label.\n",
    "\n",
    "    Functionality:\n",
    "    --------------\n",
    "    1. Reads the result CSV for each label.\n",
    "    2. Ensures labels contain only binary values (0 or 1).\n",
    "    3. Computes precision, recall, F1-score, and accuracy.\n",
    "    4. Prints and returns the evaluation results as a DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for label in labels:\n",
    "        csv_path = os.path.join(output_directory, f\"batch_files_{Drug_or_Wep}\", f\"results_batch_{label}_few-shot.csv\")\n",
    "        \n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Ensure labels are binary (0/1)\n",
    "            if not set(df['true_label'].unique()).issubset({0,1}) or not set(df['generated_label'].unique()).issubset({0,1}):\n",
    "                print(f\"Skipping {label}: Labels are not binary!\")\n",
    "                continue\n",
    "            \n",
    "            precision = precision_score(df['true_label'], df['generated_label'], average='binary', pos_label=1)\n",
    "            recall = recall_score(df['true_label'], df['generated_label'], average='binary', pos_label=1)\n",
    "            f1 = f1_score(df['true_label'], df['generated_label'], average='binary', pos_label=1)\n",
    "            accuracy = accuracy_score(df['true_label'], df['generated_label'])\n",
    "            \n",
    "            results.append({\n",
    "                'Label': label,\n",
    "                'Precision': precision,\n",
    "                'Recall': recall,\n",
    "                'F1-Score': f1,\n",
    "                'Accuracy': accuracy\n",
    "            })\n",
    "        else:\n",
    "            print(f\"File not found: {csv_path}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)\n",
    "    return results_df\n",
    "\n",
    "evaluate_results(output_directory, labels, Drug_or_Wep)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
