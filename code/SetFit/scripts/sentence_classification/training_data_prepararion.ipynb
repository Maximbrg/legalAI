{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expirement_name = 'splits_data'\n",
    "save_path = f'Code/SetFit//resources/data/drugs/{expirement_name}'\n",
    "\n",
    "train_df = '/Data/tagged_data_manualy/drugs/train_drugs.csv'\n",
    "test_df = '/Data/tagged_data_manualy/drugs/test_drugs.csv'\n",
    "eval_df = '/Data/tagged_data_manualy/drugs/eval_drugs.csv'\n",
    "\n",
    "#read the data\n",
    "train_df = pd.read_csv(train_df)\n",
    "test_df = pd.read_csv(test_df)\n",
    "eval_df = pd.read_csv(eval_df)\n",
    "\n",
    "\n",
    "# Remove the directory if it exists, then create it\n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "\n",
    "os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['reject', 'CIR_PUNISHMENT', 'CONFESSION', 'CIR_TYPE', 'CIR_ROLE',\n",
    "                  'GENERAL_CIRCUM', 'CIR_AMOUNT', 'REGRET', 'RESPO', 'CIR_EQ','CIRCUM_OFFENCE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_vounts(df):\n",
    "    verdict_grouped = df.groupby('verdict').size().reset_index(name='total_sentences')\n",
    "    label_counts = df.groupby('verdict')[label_columns].sum().reset_index()\n",
    "\n",
    "    combined_df = pd.merge(verdict_grouped, label_counts, on='verdict')\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def balance_dataframe(df):\n",
    "    \n",
    "    min_count = df['label'].value_counts().min()\n",
    "    balanced_df = df.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "def create_label_dict(df, balance=True, text_column='text', ):    \n",
    "    \n",
    "    label_dict = {}\n",
    "    for label in label_columns:\n",
    "        label_df = df[[text_column, label]]\n",
    "        label_df.rename(columns={label: 'label'}, inplace=True)\n",
    "        if balance:\n",
    "            label_df = balance_dataframe(label_df)\n",
    "        label_dict[label] = label_df\n",
    "\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "\n",
    "def save_dict_as_pickle(label_dict, file_name='label_dataframes.pkl'):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(label_dict, f)\n",
    "    print(f\"Dictionary saved as {file_name}\")\n",
    "    \n",
    "def load_dict_from_pickle(file_name='label_dataframes.pkl'):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        label_dict = pickle.load(f)\n",
    "    print(f\"Dictionary loaded from {file_name}\")\n",
    "    return label_dict\n",
    "\n",
    "def save_dfs_pickle(df, save_path, type_, balance=False):\n",
    "    label_dataframes = create_label_dict(df, balance)\n",
    "    save_dict_as_pickle(label_dataframes, os.path.join(save_path, type_ + '_label_dataframes.pkl'))\n",
    "    return label_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for drugs\n",
    "CIR_COLUMNS = ['CIR_TYPE', 'CIR_ROLE','CIR_AMOUNT','CIR_EQ']\n",
    "# for each dataframe, create new column as CIRCUM_OFFENCE and put 1 if the was at least one of the CIR_COLUMNS is 1\n",
    "def create_circum_offence_column(df):\n",
    "    df['CIRCUM_OFFENCE'] = df[CIR_COLUMNS].any(axis=1).astype(int)\n",
    "    return df\n",
    "train_df = create_circum_offence_column(train_df)\n",
    "test_df = create_circum_offence_column(test_df)\n",
    "eval_df = create_circum_offence_column(eval_df)\n",
    "\n",
    "train_pkl = save_dfs_pickle(train_df, save_path, 'train_balance', balance=True)\n",
    "test_pkl = save_dfs_pickle(test_df, save_path, 'test')\n",
    "eval_pkl = save_dfs_pickle(eval_df, save_path, 'eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weapon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expirement_name = 'splits_data'\n",
    "save_path = f'Code/SetFit//resources/data/weapon/{expirement_name}'\n",
    "\n",
    "train_df = '/Data/tagged_data_manualy/weapon/train.csv'\n",
    "test_df = '/Data/tagged_data_manualy/weapon/test.csv'\n",
    "eval_df = '/Data/tagged_data_manualy/weapon/eval.csv'\n",
    "\n",
    "#read the data\n",
    "train_df = pd.read_csv(train_df)\n",
    "test_df = pd.read_csv(test_df)\n",
    "eval_df = pd.read_csv(eval_df)\n",
    "\n",
    "\n",
    "# Remove the directory if it exists, then create it\n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "\n",
    "os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['reject', 'CONFESSION', 'CIR_TYPE_WEP', 'CIR_HELD_WAY_WEP', 'CIR_AMMU_AMOUNT_WEP','CIR_PURPOSE',\n",
    "                 'GENERAL_CIRCUM_WEP', 'CIR_STATUS_WEP', 'REGRET', 'PUNISHMENT', 'CIR_PLANNING','RESPO','CIR_OBTAIN_WAY_WEP',\n",
    "                'CICIR_USER_EQ_WEP','CIRCUM_OFFENCE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_vounts(df):\n",
    "    verdict_grouped = df.groupby('verdict').size().reset_index(name='total_sentences')\n",
    "    label_counts = df.groupby('verdict')[label_columns].sum().reset_index()\n",
    "\n",
    "    combined_df = pd.merge(verdict_grouped, label_counts, on='verdict')\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "def balance_dataframe(df):\n",
    "    \n",
    "    min_count = df['label'].value_counts().min()\n",
    "    balanced_df = df.groupby('label').apply(lambda x: x.sample(min_count)).reset_index(drop=True)\n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "def create_label_dict(df, balance=True, text_column='text', ):    \n",
    "    \n",
    "    label_dict = {}\n",
    "    for label in label_columns:\n",
    "        label_df = df[[text_column, label]]\n",
    "        label_df.rename(columns={label: 'label'}, inplace=True)\n",
    "        if balance:\n",
    "            label_df = balance_dataframe(label_df)\n",
    "        label_dict[label] = label_df\n",
    "\n",
    "    return label_dict\n",
    "\n",
    "\n",
    "\n",
    "def save_dict_as_pickle(label_dict, file_name='label_dataframes.pkl'):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(label_dict, f)\n",
    "    print(f\"Dictionary saved as {file_name}\")\n",
    "    \n",
    "def load_dict_from_pickle(file_name='label_dataframes.pkl'):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        label_dict = pickle.load(f)\n",
    "    print(f\"Dictionary loaded from {file_name}\")\n",
    "    return label_dict\n",
    "\n",
    "def save_dfs_pickle(df, save_path, type_, balance=False):\n",
    "    label_dataframes = create_label_dict(df, balance)\n",
    "    save_dict_as_pickle(label_dataframes, os.path.join(save_path, type_ + '_label_dataframes.pkl'))\n",
    "    return label_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CIRCUM_OFFENCE\n",
       "0    1363\n",
       "1     239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for drugs\n",
    "CIR_COLUMNS = ['CIR_TYPE_WEP', 'CIR_HELD_WAY_WEP', 'CIR_AMMU_AMOUNT_WEP','CIR_PURPOSE','CIR_STATUS_WEP',\n",
    "               'CIR_PLANNING','CIR_OBTAIN_WAY_WEP','CIR_USE']\n",
    "# for each dataframe, create new column as CIRCUM_OFFENCE and put 1 if the was at least one of the CIR_COLUMNS is 1\n",
    "def create_circum_offence_column(df):\n",
    "    df['CIRCUM_OFFENCE'] = df[CIR_COLUMNS].any(axis=1).astype(int)\n",
    "    return df\n",
    "train_df = create_circum_offence_column(train_df)\n",
    "test_df = create_circum_offence_column(test_df)\n",
    "eval_df = create_circum_offence_column(eval_df)\n",
    "train_df['CIRCUM_OFFENCE'].value_counts()\n",
    "train_pkl = save_dfs_pickle(train_df, save_path, 'train_balance', balance=True)\n",
    "test_pkl = save_dfs_pickle(test_df, save_path, 'test')\n",
    "eval_pkl = save_dfs_pickle(eval_df, save_path, 'eval')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tal_env_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
